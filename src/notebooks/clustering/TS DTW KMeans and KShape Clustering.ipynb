{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans and KShape Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path \n",
    "from typing import List\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "from random import randrange\n",
    "from datetime import timedelta\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf,month_plot,quarter_plot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent.parent / \"data\"\n",
    "soil_df_path = data_path / \"SOPA-data-TS-daily-clean-2023-04-01-to-2023-09-30.csv\"\n",
    "soil_df_raw = pd.read_csv(soil_df_path, index_col=0)\n",
    "soil_df = soil_df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_microseconds(datetime_str):\n",
    "    if '.' in datetime_str:\n",
    "        dot_index = datetime_str.index('.')\n",
    "        plus_index = datetime_str.index('+')\n",
    "        return datetime_str[:dot_index] + datetime_str[plus_index:]\n",
    "    else:\n",
    "        return datetime_str\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_df['datetime'] = soil_df['datetime'].apply(remove_microseconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'index' is the index column of your DataFrame\n",
    "# Replace 'column_name' with the name of the column you want to plot\n",
    "column_name = 'SENS0015-SM-SOPA'\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(soil_df.index, soil_df[column_name], marker='o', linestyle='-')\n",
    "plt.title('Daily Values of {}'.format(column_name))\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-04-01 06:00:00+00:00'\n",
    "end_date = '2023-09-30 23:00:00+00:00'\n",
    "\n",
    "soil_df = soil_df[(soil_df['datetime'] >= start_date) & (soil_df['datetime'] < end_date)]\n",
    "\n",
    "soil_df.to_csv(data_path / 'SOPA-data-raw-2023-04-01-to-2023-09-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_df=soil_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_station_names = ['SENS0004-SM-SOPA', 'SENS0005-SM-SOPA', 'SENS0006-SM-SOPA']\n",
    "dfp = df[df['station_name'].isin(specific_station_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd=df.groupby(['station_name',df.index.date])['value'].median().unstack(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_sensors = []\n",
    "for col in dfd.columns:\n",
    "    consecutive_days = 0\n",
    "    for value in dfd[col].isnull():\n",
    "        if value:\n",
    "            consecutive_days += 1\n",
    "        else:\n",
    "            consecutive_days = 0\n",
    "        if consecutive_days >= 3:\n",
    "            missing_sensors.append(col)\n",
    "            break\n",
    "            \n",
    "print(\"Sensors with missing data for 3 or more consecutive days:\", missing_sensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_cleaned = dfd.drop(columns=missing_sensors)\n",
    "\n",
    "# Optionally, you can also drop rows with missing values across all sensors if needed\n",
    "dfd_cleaned = dfd_cleaned.dropna(how='all')\n",
    "\n",
    "# Optionally, you can also reset the index if needed\n",
    "dfd_cleaned.reset_index(inplace=True)\n",
    "dfd_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_cleaned.to_csv(data_path / 'SOPA-data-TS-daily-clean-2023-04-01-to-2023-09-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd=pd.read_csv(data_path / 'SOPA-data-TS-daily-clean-2023-04-01-to-2023-09-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of sensors per row\n",
    "sensors_per_row = 3\n",
    "\n",
    "# Calculate the number of rows needed\n",
    "num_sensors = len(dfd.columns)\n",
    "num_rows = -(-num_sensors // sensors_per_row)  # Ceiling division to ensure all sensors are covered\n",
    "\n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(num_rows, sensors_per_row, figsize=(15, 5*num_rows))\n",
    "\n",
    "# Flatten the axes if there's only one row\n",
    "if num_rows == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# Iterate over columns and plot\n",
    "for i, col in enumerate(dfd.columns):\n",
    "    row_idx = i // sensors_per_row\n",
    "    col_idx = i % sensors_per_row\n",
    "    axs[row_idx][col_idx].plot(dfd.index, dfd[col])\n",
    "    axs[row_idx][col_idx].set_title(col)\n",
    "    axs[row_idx][col_idx].set_xlabel('Date')\n",
    "    axs[row_idx][col_idx].set_ylabel('Value')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for i in range(num_sensors, num_rows * sensors_per_row):\n",
    "    row_idx = i // sensors_per_row\n",
    "    col_idx = i % sensors_per_row\n",
    "    axs[row_idx][col_idx].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Unnamed: 0' column to 'station_name'\n",
    "dfd.rename(columns={'Unnamed: 0': 'station_name'}, inplace=True)\n",
    "\n",
    "# Set the 'station_name' column as the index\n",
    "dfd.set_index('station_name', inplace=True)\n",
    "\n",
    "# Drop the 'index' column if needed\n",
    "if 'index' in dfd.columns:\n",
    "    dfd.drop(columns='index', inplace=True)\n",
    "\n",
    "# Now, the DataFrame dfd has the desired structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with forward fill\n",
    "dfd = dfd.ffill(axis=0)\n",
    "\n",
    "# Now, the DataFrame dfd has missing values imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KShape clsutering\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.clustering import KShape\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming dfh is your DataFrame containing sensor data\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "# Extract the sensor data\n",
    "#dfd_imputed = dfd_cleaned.fillna(dfd.mean())\n",
    "#dfd.reset_index(inplace=True)\n",
    "#dfd['Unnamed: 0'] = pd.to_datetime(dfd['Unnamed: 0'])\n",
    "\n",
    "# Now, you can use the 'Unnamed: 0' column as the index\n",
    "#dfd.set_index('Unnamed: 0', inplace=True)\n",
    "#dfd = dfd.ffill(axis=0)\n",
    "\n",
    "sensor_data = dfd.values.T  # Transpose to have sensors as rows and time steps as columns\n",
    "\n",
    "# Step 2: Apply KShape clustering algorithm\n",
    "n_clusters = 30\n",
    "kshape = KShape(n_clusters=n_clusters, verbose=True, random_state=42)\n",
    "kshape.fit(sensor_data)\n",
    "\n",
    "# Step 3: Plot each cluster\n",
    "cluster_labels = kshape.labels_\n",
    "for cluster_id in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "    for idx in cluster_indices:\n",
    "        plt.plot(sensor_data[idx], label=f'Sensor {idx}')\n",
    "    plt.title(f'Cluster {cluster_id + 1}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Sensor Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW KMeans clustering\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming dfd is your DataFrame containing sensor data\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "# Impute missing values with forward fill\n",
    "#dfd = dfd.ffill(axis=0)\n",
    "\n",
    "# Transpose to have sensors as rows and time steps as columns\n",
    "sensor_data = dfd.values.T  # Transpose to have sensors as rows and time steps as columns\n",
    "\n",
    "# Step 2: Apply DTW clustering algorithm\n",
    "n_clusters = 30\n",
    "dtw_kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", verbose=True, random_state=42)\n",
    "dtw_kmeans.fit(sensor_data)\n",
    "\n",
    "# Step 3: Plot each cluster\n",
    "cluster_labels = dtw_kmeans.labels_\n",
    "for cluster_id in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "    for idx in cluster_indices:\n",
    "        plt.plot(sensor_data[idx], label=f'Sensor {idx}')\n",
    "    plt.title(f'Cluster {cluster_id + 1}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Sensor Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh=df.groupby([df['datetime_converted'].dt.strftime('%Y-%m-%d %H'), 'station_name'])['value'].median().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.to_csv(data_path / 'SOPA-data-TS-hourly-2023-04-01-to-2023-09-30.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dfh contains NaN values\n",
    "if dfh.isnull().values.any():\n",
    "    print(\"DataFrame contains NaN values\")\n",
    "else:\n",
    "    print(\"DataFrame does not contain NaN values\")\n",
    "\n",
    "# Print the shape and head of dfh to inspect its structure\n",
    "print(\"Shape of dfh:\", dfh.shape)\n",
    "print(\"Head of dfh:\", dfh.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.clustering import KShape\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming dfh is your DataFrame containing sensor data\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "# Extract the sensor data\n",
    "dfh_imputed = dfh.fillna(dfh.mean())\n",
    "\n",
    "\n",
    "sensor_data = dfh_imputed.values.T  # Transpose to have sensors as rows and time steps as columns\n",
    "\n",
    "# Step 2: Apply KShape clustering algorithm\n",
    "n_clusters = 10\n",
    "kshape = KShape(n_clusters=n_clusters, verbose=True, random_state=42)\n",
    "kshape.fit(sensor_data)\n",
    "\n",
    "# Step 3: Plot each cluster\n",
    "cluster_labels = kshape.labels_\n",
    "for cluster_id in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "    for idx in cluster_indices:\n",
    "        plt.plot(sensor_data[idx], label=f'Sensor {idx}')\n",
    "    plt.title(f'Cluster {cluster_id + 1}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Sensor Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.2xlarge",
  "interpreter": {
   "hash": "062d7405c1402c536d6e9cf434a31f65dabb29740fb446276f93708b09875eb6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6d1a5a2f7b59c802f952cfd4e6f1162b31861843ba4dd3579ffbbca71c6c584"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
